## Transformer_Backward_Propagation

### Introduction

This is an implementation of backward propagation of Transformer by Pytorch based on the paper: *Attention is all you need*.

### Requirements

```
torch==1.11.0
numpy==1.22.3
```

### Project Structure

```
Transformer_Backward_Propagation
	--Back Propagation
		--.idea
		--_pycache_
		--basic_layer.py
		--Encoder.py
		--FFN.py
		--LayerNorm.py
		--MultiHead.py
	--LISENCE
	--README.md
```

### Finished

- [x] FFN Layer
- [x] Linear Layer
- [x] Multi-head Attention Layer
- [x] Encoder Layer

#### To Do

- [ ] Embedding Layer
- [ ] Decoder Layer
- [ ] Encoder
- [ ] Decoder
- [ ] Transformer

**Notice**: Without considering dropout